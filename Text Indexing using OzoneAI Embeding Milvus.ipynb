{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d8240b8",
   "metadata": {},
   "source": [
    "### Install Required Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bb5d8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install below libraries if don't if you are trying for the first time.\n",
    "# !pip install langchain\n",
    "# !pip install numpy\n",
    "# !pip install pymilvus\n",
    "# !pip install requests\n",
    "# !pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98347ba4",
   "metadata": {},
   "source": [
    "### Make sure milvus service is running"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681372d5",
   "metadata": {},
   "source": [
    "#### Procedure to configure milvus\n",
    "- download docker compose`dokcer-compose.yml` using `wget https://github.com/milvus-io/milvus/releases/download/v2.2.10/milvus-standalone-docker-compose.yml -O docker-compose.yml` \n",
    "- to start service run `docker-compose up -d`\n",
    "- To stop the service `docker-compose down`\n",
    "- To check active containers `docker ps -a`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3333337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np, faiss, sqlite3, requests, os, json\n",
    "from tqdm.notebook import tqdm\n",
    "import hashlib\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util import parse_url\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "from requests.compat import urljoin\n",
    "\n",
    "# import milvus modules\n",
    "from pymilvus import (\n",
    "    connections,\n",
    "    utility,\n",
    "    FieldSchema,\n",
    "    CollectionSchema,\n",
    "    DataType,\n",
    "    Collection,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "200d6a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedder Client\n",
    "class OzoneEmbedder(object):\n",
    "    \"\"\"Ozone Embedder Client Application\"\"\"\n",
    "    def __init__(self, api_details) -> None:\n",
    "        super(OzoneEmbedder, self).__init__()\n",
    "        self.username = api_details[\"username\"]\n",
    "        self.bearer_token = api_details[\"bearer_token\"]\n",
    "        self.endpoint = api_details[\"endpoint\"]\n",
    "        self.url_details = parse_url(self.endpoint)\n",
    "        self.max_retries = 3\n",
    "        self.backoff_factor = 0.3\n",
    "\n",
    "    def connect(self):\n",
    "        # creating persistent connection\n",
    "        retries = Retry(\n",
    "            total=self.max_retries,\n",
    "            backoff_factor=self.backoff_factor\n",
    "        )\n",
    "        adapter = HTTPAdapter(max_retries=retries)\n",
    "        scheme = self.url_details.scheme\n",
    "        self.connection = requests.Session()\n",
    "        self.connection.mount(scheme, adapter)\n",
    "\n",
    "    def close(self):\n",
    "        self.connection.close()\n",
    "    \n",
    "    def __enter__(self):\n",
    "        self.connect()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        self.close()\n",
    "\n",
    "    def get_embedding(self, text, model=\"siv-sentence-bitnet-pmbv2-wikid-large\"):\n",
    "        \"\"\"\n",
    "        text: input text\n",
    "        model: \n",
    "            \"siv-sentence-bitnet-pmbv2-wikid-large\" or,\n",
    "            \"siv-sentence-bitnet-pmbv2-wikid-small\" or,\n",
    "            \"sentence-bitnet-pmbv2\"\n",
    "        \"\"\"\n",
    "        \n",
    "        headers = {\n",
    "            \"accept\": \"application/json\",\n",
    "            \"Authorization\": f\"Bearer {self.bearer_token}\",\n",
    "            \"Content-Type\": \"application/x-www-form-urlencoded\",\n",
    "        }\n",
    "\n",
    "        data = {\n",
    "            \"input_text\": text,\n",
    "            \"embedder_name\": model,\n",
    "        }\n",
    "\n",
    "        response = requests.post(\n",
    "            self.endpoint,\n",
    "            headers=headers, \n",
    "            data=data\n",
    "        )\n",
    "        return response.json()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4851a00b",
   "metadata": {},
   "source": [
    "### Load credential information from environment variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c634474",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.environ.get('OZAI_API_CREDENTIALS')) as fp:\n",
    "    credential = json.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c217280f",
   "metadata": {},
   "source": [
    "### Read text document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b124615c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_path=\"./sample.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e81957",
   "metadata": {},
   "source": [
    "### Preprocess text document using langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6d9cef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 370, which is longer than the specified 200\n",
      "Created a chunk of size 255, which is longer than the specified 200\n",
      "Created a chunk of size 487, which is longer than the specified 200\n",
      "Created a chunk of size 461, which is longer than the specified 200\n",
      "Created a chunk of size 629, which is longer than the specified 200\n",
      "Created a chunk of size 526, which is longer than the specified 200\n",
      "Created a chunk of size 545, which is longer than the specified 200\n",
      "Created a chunk of size 503, which is longer than the specified 200\n",
      "Created a chunk of size 258, which is longer than the specified 200\n",
      "Created a chunk of size 214, which is longer than the specified 200\n",
      "Created a chunk of size 352, which is longer than the specified 200\n",
      "Created a chunk of size 226, which is longer than the specified 200\n",
      "Created a chunk of size 430, which is longer than the specified 200\n",
      "Created a chunk of size 394, which is longer than the specified 200\n",
      "Created a chunk of size 257, which is longer than the specified 200\n",
      "Created a chunk of size 233, which is longer than the specified 200\n",
      "Created a chunk of size 475, which is longer than the specified 200\n",
      "Created a chunk of size 352, which is longer than the specified 200\n",
      "Created a chunk of size 272, which is longer than the specified 200\n",
      "Created a chunk of size 332, which is longer than the specified 200\n",
      "Created a chunk of size 588, which is longer than the specified 200\n",
      "Created a chunk of size 478, which is longer than the specified 200\n",
      "Created a chunk of size 271, which is longer than the specified 200\n"
     ]
    }
   ],
   "source": [
    "# load text document and split by chunk size\n",
    "# Note: Document handler can be changed based on usage (check more options https://python.langchain.com/docs/modules/data_connection/document_loaders/)\n",
    "\n",
    "# load\n",
    "text_loader = TextLoader(text_path)\n",
    "documents = text_loader.load()\n",
    "\n",
    "# split document\n",
    "text_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=10)\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee95e4cb",
   "metadata": {},
   "source": [
    "### Encoding documents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd69f08d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27564624accc44a09d8aa8bb10d092bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Encode the documents\n",
    "with OzoneEmbedder(credential) as ozone_embedder:\n",
    "    encoded_documents = np.asarray([ozone_embedder.get_embedding(d.page_content)['embedding'][0] for d in tqdm(docs)]).astype('uint8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b17f038",
   "metadata": {},
   "source": [
    "### Connect to Milvus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a2c23c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "connections.connect(\n",
    "  alias=\"default\",\n",
    "  host='localhost',\n",
    "  port='19530'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583cda5b",
   "metadata": {},
   "source": [
    "### Create a collection to index data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81bbbafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define all collection details for search and index\n",
    "collection_details = {\n",
    "    \"name\":\"search_demo\",\n",
    "    \"description\":\"Search and Retrieval Demo\",\n",
    "    \"partition_name\":\"search_app\",\n",
    "    \"index_field\":\"embeddings\",\n",
    "    \"output_field\":[\"docid\", \"texts\"],\n",
    "    \"fields\":[\"docid\", \"texts\", \"embeddings\"],\n",
    "    \"index_params\":{\"metric_type\":\"HAMMING\", \"index_type\":\"BIN_FLAT\", \"params\":{\"nlist\":10}},\n",
    "    \"search_params\":{\"metric_type\": \"HAMMING\", \"offset\": 0}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3442c28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding size: (38, 300), index dimension: 2400\n"
     ]
    }
   ],
   "source": [
    "# Encoded documents are packed bit ('unit8') then represented on binary vectors\n",
    "\n",
    "# Actual embedding dimension would be 8 times as data is uint8\n",
    "dimension = encoded_documents.shape[1] *8  # Dimension of the binary vectors\n",
    "print(f\"embedding size: {encoded_documents.shape}, index dimension: {dimension}\")\n",
    "\n",
    "# define Field Schemas\n",
    "docid = FieldSchema(\n",
    "    name=\"docid\",\n",
    "    dtype=DataType.VARCHAR,\n",
    "    max_length=64,\n",
    "    is_primary=True\n",
    ")\n",
    "\n",
    "texts = FieldSchema(\n",
    "    name=\"texts\",\n",
    "    dtype=DataType.VARCHAR,\n",
    "    max_length=1000 # change based on requirement\n",
    ")\n",
    "\n",
    "embeddings= FieldSchema(\n",
    "    name=\"embeddings\",\n",
    "    dtype=DataType.BINARY_VECTOR,\n",
    "    dim=dimension\n",
    ")\n",
    "\n",
    "# define field order (this has be same order during data insertion)\n",
    "fields = [docid, texts, embeddings]\n",
    "\n",
    "# create schema\n",
    "schema = CollectionSchema(\n",
    "  fields=fields,\n",
    "  description=collection_details[\"description\"]\n",
    ")\n",
    "\n",
    "# Create or get an existing collection.\n",
    "collection = Collection(collection_details[\"name\"], schema=schema)\n",
    "# collection = Collection(collection_name)      # Get an existing collection.\n",
    "\n",
    "# create partition if doesnt exist\n",
    "if not collection.has_partition(collection_details[\"partition_name\"]):\n",
    "    collection.create_partition(collection_details[\"partition_name\"])\n",
    "    \n",
    "# create index\n",
    "# Note: only vector fields can be indexed\n",
    "if not collection.has_index():\n",
    "    collection.create_index(\n",
    "        field_name=collection_details[\"index_field\"],\n",
    "        index_params=collection_details[\"index_params\"]\n",
    "    )\n",
    "\n",
    "    utility.index_building_progress(\"search_demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96e5cd6",
   "metadata": {},
   "source": [
    "### Update or insert data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea644ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(insert count: 38, delete count: 0, upsert count: 0, timestamp: 442492135574863876, success count: 38, err count: 0)\n"
     ]
    }
   ],
   "source": [
    "# prep data to insert\n",
    "# Note : 1) All fields should be in same sequence as schema\n",
    "#        2) Make sure data length should be same for all fields\n",
    "\n",
    "# consideration docid is made md5 of text. this can be changed based on use case\n",
    "insert_data = [\n",
    "    [hashlib.md5(d.page_content.encode()).hexdigest() for d in docs], # docid\n",
    "    [d.page_content for d in docs], # texts\n",
    "    [bytes(d.tolist()) for d in encoded_documents] # embedding as binary vector\n",
    "]\n",
    "\n",
    "insert_ack = collection.insert(insert_data) # insert/update data to milvus (it upfdate)\n",
    "collection.flush() # make sure you persist the data\n",
    "print(insert_ack)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc3ab77",
   "metadata": {},
   "source": [
    "### Search Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f7a0b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            Query: **Welcome To The World of Free Plain Vanilla Electronic Texts**\n",
      "\n",
      "**Etexts Readable By Both Humans and By Computers, Since 1971**\n",
      "\n",
      "*These Etexts Prepared By Hundreds of Volunteers and Donations*\n",
      "\n",
      "            --------------------------\n",
      "            Closest [0], DocID [4cb5885f975ad37c3b062f44b9493f76]:\n",
      "\n",
      "            Text: **Welcome To The World of Free Plain Vanilla Electronic Texts**\n",
      "\n",
      "**Etexts Readable By Both Humans and By Computers, Since 1971**\n",
      "\n",
      "*These Etexts Prepared By Hundreds of Volunteers and Donations*\n",
      "\n",
      "            xxxxxxx\n",
      "            \n",
      "\n",
      "            Query: **Welcome To The World of Free Plain Vanilla Electronic Texts**\n",
      "\n",
      "**Etexts Readable By Both Humans and By Computers, Since 1971**\n",
      "\n",
      "*These Etexts Prepared By Hundreds of Volunteers and Donations*\n",
      "\n",
      "            --------------------------\n",
      "            Closest [1], DocID [e61a668b8f3ccaa05c9da12d345d50d4]:\n",
      "\n",
      "            Text: [*]  The etext may be readily converted by the reader at\n",
      "          no expense into plain ASCII, EBCDIC or equivalent\n",
      "          form by the program that displays the etext (as is\n",
      "          the case, for instance, with most word processors);\n",
      "          OR\n",
      "\n",
      "            xxxxxxx\n",
      "            \n",
      "\n",
      "            Query: **Welcome To The World of Free Plain Vanilla Electronic Texts**\n",
      "\n",
      "**Etexts Readable By Both Humans and By Computers, Since 1971**\n",
      "\n",
      "*These Etexts Prepared By Hundreds of Volunteers and Donations*\n",
      "\n",
      "            --------------------------\n",
      "            Closest [2], DocID [2947ca444f2e9a5eff33e2446decdb44]:\n",
      "\n",
      "            Text: [1]  Only give exact copies of it.  Among other things, this\n",
      "     requires that you do not remove, alter or modify the\n",
      "     etext or this \"small print!\" statement.  You may however,\n",
      "     if you wish, distribute this etext in machine readable\n",
      "     binary, compressed, mark-up, or proprietary form,\n",
      "     including any form resulting from conversion by word pro-\n",
      "     cessing or hypertext software, but only so long as\n",
      "     *EITHER*:\n",
      "\n",
      "            xxxxxxx\n",
      "            \n",
      "\n",
      "            Query: **Welcome To The World of Free Plain Vanilla Electronic Texts**\n",
      "\n",
      "**Etexts Readable By Both Humans and By Computers, Since 1971**\n",
      "\n",
      "*These Etexts Prepared By Hundreds of Volunteers and Donations*\n",
      "\n",
      "            --------------------------\n",
      "            Closest [3], DocID [7db43ceaa77979cabe84aa584530d843]:\n",
      "\n",
      "            Text: [*]  You provide, or agree to also provide on request at\n",
      "          no additional cost, fee or expense, a copy of the\n",
      "          etext in its original plain ASCII form (or in EBCDIC\n",
      "          or other equivalent proprietary form).\n",
      "\n",
      "            xxxxxxx\n",
      "            \n",
      "\n",
      "            Query: **Welcome To The World of Free Plain Vanilla Electronic Texts**\n",
      "\n",
      "**Etexts Readable By Both Humans and By Computers, Since 1971**\n",
      "\n",
      "*These Etexts Prepared By Hundreds of Volunteers and Donations*\n",
      "\n",
      "            --------------------------\n",
      "            Closest [4], DocID [2c07854db8837e3b74a84d5278a08a22]:\n",
      "\n",
      "            Text: ***START**THE SMALL PRINT!**FOR PUBLIC DOMAIN ETEXTS**START***\n",
      "Why is this \"Small Print!\" statement here?  You know: lawyers.\n",
      "They tell us you might sue us if there is something wrong with\n",
      "your copy of this etext, even if you got it for free from\n",
      "someone other than us, and even if what's wrong is not our\n",
      "fault.  So, among other things, this \"Small Print!\" statement\n",
      "disclaims most of our liability to you.  It also tells you how\n",
      "you can distribute copies of this etext if you want to.\n",
      "\n",
      "            xxxxxxx\n",
      "            \n"
     ]
    }
   ],
   "source": [
    "# query always happens on memory. so make sure for runtime search you must load the collection separately\n",
    "query_collection = Collection(collection_details[\"name\"])\n",
    "query_collection.load()\n",
    "\n",
    "# Perform a search on the index\n",
    "# query = \"what was the U. S. Bill of Rights\"\n",
    "query = docs[0].page_content\n",
    "\n",
    "with OzoneEmbedder(credential) as ozone_embedder:\n",
    "    encoded_query = ozone_embedder.get_embedding(query)['embedding']\n",
    "    query_embedding = [bytes(i) for i in np.asarray(encoded_query).astype('uint8')]\n",
    "    \n",
    "    results = query_collection.search(\n",
    "        data=query_embedding, \n",
    "        anns_field=collection_details[\"index_field\"], \n",
    "        param=collection_details[\"search_params\"],\n",
    "        limit=5,\n",
    "        expr=None,\n",
    "        output_fields=collection_details[\"output_field\"] ,# set the names of the fields you want to retrieve from the search result.\n",
    "        consistency_level=\"Strong\"\n",
    "    )\n",
    "    \n",
    "    ids_list=results[0].ids\n",
    "    ed_distance=results[0].distances\n",
    "\n",
    "    result_dict_list = []\n",
    "\n",
    "    for index, ids in enumerate(ids_list):\n",
    "        hit = results[0][index]\n",
    "        result_dict = {\n",
    "            \"ids\": ids,\n",
    "            \"score\": hit.score,\n",
    "            \"distance\": hit.distance,\n",
    "            \"docid\": hit.entity.get('docid'),\n",
    "            \"texts\": hit.entity.get('texts')\n",
    "\n",
    "        }\n",
    "        print(f\"\"\"\n",
    "            Query: {query}\\n\n",
    "            --------------------------\n",
    "            Closest [{index}], DocID [{result_dict['docid']}]:\\n\n",
    "            Text: {result_dict['texts']}\n",
    "\n",
    "            xxxxxxx\n",
    "            \"\"\")\n",
    "        result_dict_list.append(result_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2bdecf",
   "metadata": {},
   "source": [
    "### Deletion of collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b530a53f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(insert count: 0, delete count: 1, upsert count: 0, timestamp: 442492141590806529, success count: 0, err count: 0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # To drop partition run below\n",
    "# collection.release()\n",
    "# collection.drop_partition(collection_details['partition_name'])\n",
    "\n",
    "# # To drop collection run below \n",
    "# utility.drop_collection(collection_details[\"name\"])\n",
    "\n",
    "\n",
    "# # To delete particular data by docid\n",
    "expr = \"\"\"docid in [\"4cb5885f975ad37c3b062f44b9493f76\"]\"\"\"\n",
    "collection.delete(expr=expr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
