{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d8240b8",
   "metadata": {},
   "source": [
    "### Install Required Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bb5d8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # install below libraries if don't if you are trying for the first time.\n",
    "# !pip install langchain\n",
    "# !pip install numpy\n",
    "# !pip install faiss-cpu\n",
    "# !pip install requests\n",
    "# !pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3333337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np, faiss, sqlite3, requests, os, json\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util import parse_url\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "from requests.compat import urljoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "200d6a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedder Client\n",
    "class OzoneEmbedder(object):\n",
    "    \"\"\"Ozone Embedder Client Application\"\"\"\n",
    "    def __init__(self, api_details) -> None:\n",
    "        super(OzoneEmbedder, self).__init__()\n",
    "        self.username = api_details[\"username\"]\n",
    "        self.bearer_token = api_details[\"bearer_token\"]\n",
    "        self.endpoint = api_details[\"endpoint\"]\n",
    "        self.url_details = parse_url(self.endpoint)\n",
    "        self.max_retries = 3\n",
    "        self.backoff_factor = 0.3\n",
    "\n",
    "    def connect(self):\n",
    "        # creating persistent connection\n",
    "        retries = Retry(\n",
    "            total=self.max_retries,\n",
    "            backoff_factor=self.backoff_factor\n",
    "        )\n",
    "        adapter = HTTPAdapter(max_retries=retries)\n",
    "        scheme = self.url_details.scheme\n",
    "        self.connection = requests.Session()\n",
    "        self.connection.mount(scheme, adapter)\n",
    "\n",
    "    def close(self):\n",
    "        self.connection.close()\n",
    "    \n",
    "    def __enter__(self):\n",
    "        self.connect()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        self.close()\n",
    "\n",
    "    def get_embedding(self, text, model=\"siv-sentence-bitnet-pmbv2-wikid-large\"):\n",
    "        \"\"\"\n",
    "        text: input text\n",
    "        model: \n",
    "            \"siv-sentence-bitnet-pmbv2-wikid-large\" or,\n",
    "            \"siv-sentence-bitnet-pmbv2-wikid-small\" or,\n",
    "            \"sentence-bitnet-pmbv2\"\n",
    "        \"\"\"\n",
    "        \n",
    "        headers = {\n",
    "            \"accept\": \"application/json\",\n",
    "            \"Authorization\": f\"Bearer {self.bearer_token}\",\n",
    "            \"Content-Type\": \"application/x-www-form-urlencoded\",\n",
    "        }\n",
    "\n",
    "        data = {\n",
    "            \"input_text\": text,\n",
    "            \"embedder_name\": model,\n",
    "        }\n",
    "\n",
    "        response = requests.post(\n",
    "            self.endpoint,\n",
    "            headers=headers, \n",
    "            data=data\n",
    "        )\n",
    "        return response.json()\n",
    "    \n",
    "class DocumentDatabase:\n",
    "    def __init__(self, db_file):\n",
    "        self.db_file = db_file\n",
    "\n",
    "    def _create_table(self):\n",
    "        self.cursor.execute('''CREATE TABLE IF NOT EXISTS documents\n",
    "                              (id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                               content TEXT,\n",
    "                               UNIQUE(id) ON CONFLICT IGNORE)''')\n",
    "        self.cursor.execute('CREATE INDEX IF NOT EXISTS idx_id ON documents (id)')\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.conn = sqlite3.connect(self.db_file)\n",
    "        self.cursor = self.conn.cursor()\n",
    "        self._create_table()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        self.conn.close()\n",
    "        \n",
    "    def insert_document(self, document):\n",
    "        self.cursor.execute(\"INSERT INTO documents (content) VALUES (?)\", (document,))\n",
    "        self.conn.commit()\n",
    "\n",
    "    def select_documents(self, query):\n",
    "        self.cursor.execute(query)\n",
    "        return self.cursor.fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4851a00b",
   "metadata": {},
   "source": [
    "### Load credential information from environment variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c634474",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.environ.get('OZAI_API_CREDENTIALS')) as fp:\n",
    "    credential = json.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c217280f",
   "metadata": {},
   "source": [
    "### Read text document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b124615c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_path=\"./sample.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e81957",
   "metadata": {},
   "source": [
    "### Preprocess text document using langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6d9cef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 370, which is longer than the specified 200\n",
      "Created a chunk of size 255, which is longer than the specified 200\n",
      "Created a chunk of size 487, which is longer than the specified 200\n",
      "Created a chunk of size 461, which is longer than the specified 200\n",
      "Created a chunk of size 629, which is longer than the specified 200\n",
      "Created a chunk of size 526, which is longer than the specified 200\n",
      "Created a chunk of size 545, which is longer than the specified 200\n",
      "Created a chunk of size 503, which is longer than the specified 200\n",
      "Created a chunk of size 258, which is longer than the specified 200\n",
      "Created a chunk of size 214, which is longer than the specified 200\n",
      "Created a chunk of size 352, which is longer than the specified 200\n",
      "Created a chunk of size 226, which is longer than the specified 200\n",
      "Created a chunk of size 430, which is longer than the specified 200\n",
      "Created a chunk of size 394, which is longer than the specified 200\n",
      "Created a chunk of size 257, which is longer than the specified 200\n",
      "Created a chunk of size 233, which is longer than the specified 200\n",
      "Created a chunk of size 475, which is longer than the specified 200\n",
      "Created a chunk of size 352, which is longer than the specified 200\n",
      "Created a chunk of size 272, which is longer than the specified 200\n",
      "Created a chunk of size 332, which is longer than the specified 200\n",
      "Created a chunk of size 588, which is longer than the specified 200\n",
      "Created a chunk of size 478, which is longer than the specified 200\n",
      "Created a chunk of size 271, which is longer than the specified 200\n"
     ]
    }
   ],
   "source": [
    "# load text document and split by chunk size\n",
    "# Note: Document handler can be changed based on usage (check more options https://python.langchain.com/docs/modules/data_connection/document_loaders/)\n",
    "\n",
    "# load\n",
    "text_loader = TextLoader(text_path)\n",
    "documents = text_loader.load()\n",
    "\n",
    "# split document\n",
    "text_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=10)\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee95e4cb",
   "metadata": {},
   "source": [
    "### Encoding documents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd69f08d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "545c92b276ba4b6c8b6e774b9b08a98c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Encode the documents\n",
    "with OzoneEmbedder(credential) as ozone_embedder:\n",
    "    encoded_documents = np.asarray([ozone_embedder.get_embedding(d.page_content)['embedding'][0] for d in tqdm(docs)]).astype('uint8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a758fa",
   "metadata": {},
   "source": [
    "### Create index using Faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23e230ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding size: (38, 300)\n"
     ]
    }
   ],
   "source": [
    "# Encoded documents are packed bit ('unit8')\n",
    "# make sure it fits to your RAM\n",
    "print(f\"embedding size: {encoded_documents.shape}\")\n",
    "\n",
    "# Actual embedding dimension would be 8 times as data is uint8\n",
    "\n",
    "dimension = encoded_documents.shape[1] * 8  # Dimension of the binary vectors\n",
    "\n",
    "# Create faiss binary index\n",
    "index = faiss.IndexBinaryFlat(dimension)\n",
    "\n",
    "# Add the binary vectors to the index\n",
    "# Note: avoid duplicate data insert\n",
    "index.add(encoded_documents)\n",
    "\n",
    "# persist data and text\n",
    "faiss.write_index_binary(index, 'index.dat')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30798d73",
   "metadata": {},
   "source": [
    "### Storing document in sqlite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02a36087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db6c60868689472a8c1e788696e8e382",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with DocumentDatabase('index.db') as conn:\n",
    "    \n",
    "    # Insert the documents into the database\n",
    "    for doc in tqdm(docs):\n",
    "        conn.insert_document(doc.page_content)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1143a32e",
   "metadata": {},
   "source": [
    "### Query Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33f80555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Query: what was the U. S. Bill of Rights\n",
      "\n",
      "        --------------------------\n",
      "        Closest [0], DocID [36]:\n",
      "\n",
      "        Text: VIII\n",
      "\n",
      "Excessive bail shall not be required nor excessive fines imposed,\n",
      "nor cruel and unusual punishments inflicted.\n",
      "\n",
      "\n",
      "IX\n",
      "        \n",
      "        xxxxxxx\n",
      "        \n",
      "\n",
      "        Query: what was the U. S. Bill of Rights\n",
      "\n",
      "        --------------------------\n",
      "        Closest [1], DocID [24]:\n",
      "\n",
      "        Text: ***\n",
      "\n",
      "These original Project Gutenberg Etexts will be compiled into a file\n",
      "containing them all, in order to improve the content ratios of Etext\n",
      "to header material.\n",
      "\n",
      "***\n",
      "        \n",
      "        xxxxxxx\n",
      "        \n",
      "\n",
      "        Query: what was the U. S. Bill of Rights\n",
      "\n",
      "        --------------------------\n",
      "        Closest [2], DocID [21]:\n",
      "\n",
      "        Text: WHAT IF YOU *WANT* TO SEND MONEY EVEN IF YOU DON'T HAVE TO?\n",
      "The Project gratefully accepts contributions in money, time,\n",
      "scanning machines, OCR software, public domain etexts, royalty\n",
      "free copyright licenses, and every other sort of contribution\n",
      "you can think of.  Money should be paid to \"Project Gutenberg\n",
      "Association / Illinois Benedictine College\".\n",
      "        \n",
      "        xxxxxxx\n",
      "        \n",
      "\n",
      "        Query: what was the U. S. Bill of Rights\n",
      "\n",
      "        --------------------------\n",
      "        Closest [3], DocID [27]:\n",
      "\n",
      "        Text: II\n",
      "\n",
      "A well-regulated militia, being necessary to the security of a free State,\n",
      "the right of the people to keep and bear arms, shall not be infringed.\n",
      "        \n",
      "        xxxxxxx\n",
      "        \n",
      "\n",
      "        Query: what was the U. S. Bill of Rights\n",
      "\n",
      "        --------------------------\n",
      "        Closest [4], DocID [26]:\n",
      "\n",
      "        Text: Congress shall make no law respecting an establishment of religion,\n",
      "or prohibiting the free exercise thereof; or abridging the freedom of speech,\n",
      "or of the press, or the right of the people peaceably to assemble,\n",
      "and to petition the Government for a redress of grievances.\n",
      "        \n",
      "        xxxxxxx\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Perform a search on the index\n",
    "query = \"what was the U. S. Bill of Rights\"\n",
    "\n",
    "with OzoneEmbedder(credential) as ozone_embedder, DocumentDatabase('index.db') as conn:\n",
    "    encoded_query = ozone_embedder.get_embedding(query)['embedding']\n",
    "\n",
    "    # Convert the query vector to a uint8 binary vector\n",
    "    xq = np.asarray(encoded_query).astype('uint8')\n",
    "    D, I = index.search(xq, k=5)  # Retrieve top 5 most similar documents\n",
    "\n",
    "    selected_data = [conn.select_documents(f\"select * from documents where id={i};\")[0] for i in I[0]]\n",
    "    for i, s in enumerate(selected_data):\n",
    "        print(f\"\"\"\n",
    "        Query: {query}\\n\n",
    "        --------------------------\n",
    "        Closest [{i}], DocID [{s[0]}]:\\n\n",
    "        Text: {s[1]}\n",
    "        \n",
    "        xxxxxxx\n",
    "        \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bd67f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
